{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown==4.6.0\n",
        "!pip install scikit-learn==1.0\n",
        "!pip install scipy==1.7.1"
      ],
      "metadata": {
        "id": "Bj8If6Nao2la"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wh9cvaAa_cE"
      },
      "outputs": [],
      "source": [
        "!gdown 1RR4VvIQ6jiBneVuPXwdq1E5FK8USI_-U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVfe4kJsa_cF"
      },
      "outputs": [],
      "source": [
        "!tar -zxvf finger_in_the_power_datasets.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfHidXMca_b9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from scipy.stats import skew, entropy, median_abs_deviation\n",
        "from scipy.signal import find_peaks\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "PATHS = {\n",
        "    'Desk-4590': 'finger_in_the_power_datasets/Desk-4590.csv',\n",
        "    'Srv-2630-v3': 'finger_in_the_power_datasets/Srv-2630-v3.csv',\n",
        "    'Srv-2630L-v4': 'finger_in_the_power_datasets/Srv-2630L-v4.csv',\n",
        "    'Srv-5220': 'finger_in_the_power_datasets/Srv-5220.csv',\n",
        "    'Srv-6130': 'finger_in_the_power_datasets/Srv-6130.csv',\n",
        "    'Srv-AMD': 'finger_in_the_power_datasets/Srv-AMD.csv'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3_-4kKha_cF"
      },
      "outputs": [],
      "source": [
        "class Clipper(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self, low_quantile, high_quantile):\n",
        "        self.low_quantile = low_quantile\n",
        "        self.high_quantile = high_quantile\n",
        "\n",
        "        self.low_quantile_val = None\n",
        "        self.high_quantile_val = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.low_quantile_val = np.quantile(X, self.low_quantile)\n",
        "        self.high_quantile_val = np.quantile(X, self.high_quantile)\n",
        "    \n",
        "    def transform(self, X, y=None):\n",
        "        X = np.copy(X)\n",
        "        X[X < self.low_quantile_val] = self.low_quantile_val\n",
        "        X[X > self.high_quantile_val] = self.high_quantile_val\n",
        "\n",
        "        return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMXn5T-Qa_cG"
      },
      "outputs": [],
      "source": [
        "def mean_temps(df):\n",
        "    power_consumption_clms = [clm for clm in df.columns if 'power_consumption' in clm] + ['machine']\n",
        "    temp_clms = [clm for clm in df.columns if 'temperature' in clm]\n",
        "    \n",
        "    temperatures = df[temp_clms].mean(axis=1)\n",
        "    df = df[power_consumption_clms].copy()\n",
        "    df['temperature'] = temperatures\n",
        "        \n",
        "    return df\n",
        "\n",
        "def get_consumption_with_temp_dfs(dfs):\n",
        "    power_consumption_clms = [clm for clm in dfs[0].columns if 'power_consumption' in clm] + ['machine']\n",
        "    temp_clms = [clm for clm in dfs[0].columns if 'temperature' in clm]\n",
        "    \n",
        "    dfs_of_consumption = [df[power_consumption_clms] for df in dfs]\n",
        "    dfs_of_temp = [df[temp_clms] for df in dfs]\n",
        "    \n",
        "    dfs_consumption_and_temp = []\n",
        "    for consumption_df, temp_df in zip(dfs_of_consumption, dfs_of_temp):\n",
        "        df = consumption_df.copy()\n",
        "        df['temperature'] = temp_df.mean(axis=1).values\n",
        "        dfs_consumption_and_temp.append(df)\n",
        "        \n",
        "    return dfs_consumption_and_temp\n",
        " \n",
        " \n",
        "def generate_trace_level_fetures(X):\n",
        "    \n",
        "    def get_sequence_of_diffs(trace, step_size):\n",
        "        left_side_idxs = np.arange(0, trace.shape[0] - step_size, step_size)\n",
        "        right_side_idx = np.arange(step_size, trace.shape[0], step_size)\n",
        "\n",
        "        sequence_of_diffs = np.abs(trace[left_side_idxs] - trace[right_side_idx])\n",
        "        return sequence_of_diffs\n",
        "\n",
        "    features = {}\n",
        "    features['mean'] = np.mean(X, axis=1)\n",
        "    features['std'] = np.std(X, axis=1)\n",
        "    features['skew'] = skew(X, axis=1)\n",
        "    features['entropy'] = entropy(X, axis=1)\n",
        "\n",
        "    features['precentile_10'] = np.percentile(X, 10, axis=1)\n",
        "    features['precentile_20'] = np.percentile(X, 20, axis=1)\n",
        "    features['precentile_30'] = np.percentile(X, 30, axis=1)\n",
        "    features['precentile_40'] = np.percentile(X, 40, axis=1)\n",
        "    features['precentile_50'] = np.percentile(X, 50, axis=1)\n",
        "    features['precentile_60'] = np.percentile(X, 60, axis=1)\n",
        "    features['precentile_70'] = np.percentile(X, 70, axis=1)\n",
        "    features['precentile_80'] = np.percentile(X, 80, axis=1)\n",
        "    features['precentile_90'] = np.percentile(X, 90, axis=1)\n",
        "\n",
        "    features['dist_mean_from_median'] = np.abs(features['mean'] - features['precentile_50'])\n",
        "    features['num_of_peaks'] = []\n",
        "\n",
        "    features['median_abs_deviation'] = median_abs_deviation(X, axis=1)\n",
        "\n",
        "    features['diff_seq_mean'] = []\n",
        "    features['diff_seq_median'] = []\n",
        "    features['diff_seq_std'] = []\n",
        "    \n",
        "    for trace in tqdm(X):\n",
        "        peaks = find_peaks(trace)[0]\n",
        "        features['num_of_peaks'].append(len(peaks))\n",
        "\n",
        "        sequence_of_diffs = get_sequence_of_diffs(trace, 1)\n",
        "        features['diff_seq_mean'].append(np.mean(sequence_of_diffs))\n",
        "        features['diff_seq_median'].append(np.median(sequence_of_diffs))\n",
        "        features['diff_seq_std'].append(np.std(sequence_of_diffs))\n",
        "\n",
        "    return pd.DataFrame(features).values\n",
        "\n",
        "\n",
        "def train_test_split_by_time(X, y, test_size):\n",
        "    X_train, X_test = [], []\n",
        "    y_train, y_test = [], []\n",
        "    for lbl in np.unique(y):\n",
        "        indices_of_lbl = y == lbl\n",
        "        X_of_lbl = X[indices_of_lbl]\n",
        "        y_of_lbl = y[indices_of_lbl]\n",
        "\n",
        "        first_idx_of_train = int((1 - test_size) * X_of_lbl.shape[0])\n",
        "        X_train_of_lbl = X_of_lbl[:first_idx_of_train]\n",
        "        X_test_of_lbl = X_of_lbl[first_idx_of_train:]\n",
        "        y_train_of_lbl = y_of_lbl[:first_idx_of_train]\n",
        "        y_test_of_lbl = y_of_lbl[first_idx_of_train:]\n",
        "\n",
        "        X_train.append(X_train_of_lbl)\n",
        "        X_test.append(X_test_of_lbl)\n",
        "        y_train.append(y_train_of_lbl)\n",
        "        y_test.append(y_test_of_lbl)\n",
        "    \n",
        "    X_train = pd.DataFrame(np.concatenate(X_train), columns=X.columns)\n",
        "    X_test = pd.DataFrame(np.concatenate(X_test), columns=X.columns)\n",
        "    y_train = pd.Series(np.concatenate(y_train))\n",
        "    y_test = pd.Series(np.concatenate(y_test))\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def from_df_into_X_y(df, has_temperature=True):\n",
        "    \"\"\"\n",
        "    for every trace, mean the temperature of all its samples\n",
        "    split into train and test\n",
        "    train a clipper on the train, apply it on all the data\n",
        "    \"\"\"\n",
        "    # train a clipper on the train part of the first df and apply it to all the data\n",
        "    # returns a list of (X, y) tuples.\n",
        "    # The first is train, the second is test of first collection, the third and later are the whole collection per tuple\n",
        "    \n",
        "    if has_temperature:\n",
        "        df = mean_temps(df)    \n",
        "    X_train, X_test, y_train, y_test = train_test_split_by_time(df.drop(['machine'], axis=1),\n",
        "                                                                df['machine'],\n",
        "                                                                0.2)\n",
        "\n",
        "    if has_temperature:\n",
        "        X_consumption_train = X_train.drop(['temperature'], axis=1)\n",
        "        X_consumption_test = X_test.drop(['temperature'], axis=1)\n",
        "    else:\n",
        "        X_consumption_train = X_train\n",
        "        X_consumption_test = X_test\n",
        "    \n",
        "    clipper = Clipper(0.01, 0.99)\n",
        "    clipper.fit(X_consumption_train)\n",
        "    \n",
        "    X_consumption_train = clipper.transform(X_consumption_train)\n",
        "    X_consumption_test = clipper.transform(X_consumption_test)\n",
        "    \n",
        "    X_train_features = generate_trace_level_fetures(X_consumption_train)\n",
        "    X_test_features = generate_trace_level_fetures(X_consumption_test)\n",
        "    \n",
        "    if has_temperature:\n",
        "        X_train_features = np.hstack([X_train_features, X_train['temperature'].values.reshape(-1, 1)])\n",
        "        X_test_features = np.hstack([X_test_features, X_test['temperature'].values.reshape(-1, 1)])\n",
        "    \n",
        "    return X_train_features, y_train, X_test_features, y_test\n",
        "\n",
        "def train_classifier_and_report(X_train, y_train, X_test, y_test, use_temperature, plot_conf_mat, group_name):\n",
        "    # the last clm is the temperature clm   \n",
        "    if not use_temperature:\n",
        "        X_train = X_train[:, :-1]\n",
        "        X_test = X_test[:, :-1]\n",
        "    \n",
        "    clf = RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1)\n",
        "    clf.fit(X_train, y_train)\n",
        "    \n",
        "    y_pred = clf.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    if plot_conf_mat:\n",
        "      conf_mat_labels = sorted(clf.classes_)\n",
        "      conf_mat = confusion_matrix(y_test, y_pred, labels=conf_mat_labels, normalize='true')\n",
        "\n",
        "      plt.figure(figsize=(10, 7))\n",
        "      ax = sns.heatmap(conf_mat,\n",
        "                      cmap=sns.cubehelix_palette(start=2, rot=0, dark=0.4, light=1, as_cmap=True))\n",
        "      \n",
        "      for _, spine in ax.spines.items():\n",
        "          spine.set_visible(True)\n",
        "\n",
        "      plt.xticks([])\n",
        "      plt.yticks([])\n",
        "\n",
        "      plt.title(f'Confusion Matrix for {group_name}')\n",
        "      plt.show()\n",
        "    \n",
        "    return clf, acc\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9-Fm26Ga_cJ"
      },
      "outputs": [],
      "source": [
        "for group_name, path in PATHS.items():\n",
        "    print(group_name)\n",
        "    df = pd.read_csv(path)\n",
        "    base_rate = 1 / df['machine'].nunique()\n",
        "    \n",
        "    has_temperature = True\n",
        "    if group_name == 'Srv-AMD':\n",
        "        has_temperature = False\n",
        "    \n",
        "    X_train, y_train, X_test, y_test = from_df_into_X_y(df, has_temperature)\n",
        "    \n",
        "    acc_with_temp = None\n",
        "    if has_temperature:\n",
        "        clf_with_temp, acc_with_temp = train_classifier_and_report(X_train, y_train, X_test, y_test, use_temperature=True, plot_conf_mat=True, group_name=group_name)\n",
        "\n",
        "    clf_without_temp, acc_without_temp = train_classifier_and_report(X_train, y_train, X_test, y_test, use_temperature=False, plot_conf_mat=not has_temperature, group_name=group_name)\n",
        "    print(f'Group {group_name}, acc with temperature: {acc_with_temp}, acc without temperature: {acc_without_temp}, base rate: {base_rate}')\n",
        "    print('-----------')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": ""
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}